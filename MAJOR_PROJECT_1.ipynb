{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqXXyjUXjBLlgClzUG+X9K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01f2d5d3e38c4aabaa8496b43ec4264a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9104a65b05e14d66b49e60cd558c92f1",
              "IPY_MODEL_edf0d7a544274c55bb2347ec509b5a9b",
              "IPY_MODEL_a34f690719b54de6b27f2d293c853cc9"
            ],
            "layout": "IPY_MODEL_7872c0067bf44c0db79b3b0f663af5ef"
          }
        },
        "9104a65b05e14d66b49e60cd558c92f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51fa495a46a049679d9eecc9ac00cfe7",
            "placeholder": "​",
            "style": "IPY_MODEL_9079f7ad68d444aabfcf2527d611ecb4",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "edf0d7a544274c55bb2347ec509b5a9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d3410e6b16f4c5697e6c13c2496a36f",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ab666f866eb241a5b0d9898143f37551",
            "value": 48
          }
        },
        "a34f690719b54de6b27f2d293c853cc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb7f590842874b57be6602f106a79365",
            "placeholder": "​",
            "style": "IPY_MODEL_f3f6e78931534089bc64490833d071ac",
            "value": " 48.0/48.0 [00:00&lt;00:00, 2.43kB/s]"
          }
        },
        "7872c0067bf44c0db79b3b0f663af5ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51fa495a46a049679d9eecc9ac00cfe7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9079f7ad68d444aabfcf2527d611ecb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d3410e6b16f4c5697e6c13c2496a36f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab666f866eb241a5b0d9898143f37551": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb7f590842874b57be6602f106a79365": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3f6e78931534089bc64490833d071ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1dd3fef1c26460abb1a99855068155e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b1e3791d3a094fd480d7955ae1b9369f",
              "IPY_MODEL_71405156e1904ebe892a49ab87e5bb87",
              "IPY_MODEL_e16fed2f40fe4bcb98af886f67076961"
            ],
            "layout": "IPY_MODEL_ba3e1bf734f84b3f8de22a80f47d4dca"
          }
        },
        "b1e3791d3a094fd480d7955ae1b9369f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0eb759e5f6b14045afce5f08d689f6b5",
            "placeholder": "​",
            "style": "IPY_MODEL_b947ec9561004cb0b396ce2a598d1621",
            "value": "vocab.txt: 100%"
          }
        },
        "71405156e1904ebe892a49ab87e5bb87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3ffb39c30de4ce781e2671fc9ddfaab",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9fa4d6c0d5df43c4acc577c4a5c48aed",
            "value": 231508
          }
        },
        "e16fed2f40fe4bcb98af886f67076961": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8325d6409b26494d8a4c9bcabbeb3518",
            "placeholder": "​",
            "style": "IPY_MODEL_04fa6f6e96e3459c8d36b7919a21984f",
            "value": " 232k/232k [00:00&lt;00:00, 2.39MB/s]"
          }
        },
        "ba3e1bf734f84b3f8de22a80f47d4dca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0eb759e5f6b14045afce5f08d689f6b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b947ec9561004cb0b396ce2a598d1621": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3ffb39c30de4ce781e2671fc9ddfaab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fa4d6c0d5df43c4acc577c4a5c48aed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8325d6409b26494d8a4c9bcabbeb3518": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04fa6f6e96e3459c8d36b7919a21984f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce7e169a4bb04ed7885449c8de213f0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5be02c2aa70f41f88e5ee5d32cf2c45d",
              "IPY_MODEL_9483c7ce076540b384e5bf7e4219d107",
              "IPY_MODEL_a412ed9c10884f4caa5c4394111dfc40"
            ],
            "layout": "IPY_MODEL_f1ec8033c2c548b49c68a2a8716b24ed"
          }
        },
        "5be02c2aa70f41f88e5ee5d32cf2c45d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b9e5c8f18244a5491b6ab192dfd8a56",
            "placeholder": "​",
            "style": "IPY_MODEL_297557a65b9a42fc8686186311dc8cfc",
            "value": "tokenizer.json: 100%"
          }
        },
        "9483c7ce076540b384e5bf7e4219d107": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03d8723f30d64f1cbffac52e706d89d7",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1cc2b2012d0a4b5d94bd23ba970fc94f",
            "value": 466062
          }
        },
        "a412ed9c10884f4caa5c4394111dfc40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19a41dd6327341169ed4b6a301d6c585",
            "placeholder": "​",
            "style": "IPY_MODEL_cdccb6d2aa0a4f7d8b5d4dcdbc0a6e43",
            "value": " 466k/466k [00:00&lt;00:00, 8.56MB/s]"
          }
        },
        "f1ec8033c2c548b49c68a2a8716b24ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b9e5c8f18244a5491b6ab192dfd8a56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "297557a65b9a42fc8686186311dc8cfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03d8723f30d64f1cbffac52e706d89d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cc2b2012d0a4b5d94bd23ba970fc94f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "19a41dd6327341169ed4b6a301d6c585": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdccb6d2aa0a4f7d8b5d4dcdbc0a6e43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b9f6f5cab044cddadf467eaca2b3d6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c3ca284e91cf4034a4d8ca93edb09675",
              "IPY_MODEL_a76ebbe6634d459ab31b1e70d5b63bd5",
              "IPY_MODEL_96bab606dfc9471f9bc00281640e50f9"
            ],
            "layout": "IPY_MODEL_ccb2770088834a7ebf9f0e1859f19827"
          }
        },
        "c3ca284e91cf4034a4d8ca93edb09675": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ab215df3a5c48cba8b438b993980610",
            "placeholder": "​",
            "style": "IPY_MODEL_9d9ada0bbaa24935bb3956e6f8316b18",
            "value": "config.json: 100%"
          }
        },
        "a76ebbe6634d459ab31b1e70d5b63bd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08043920db32475ca938101666cd1600",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5843f402223f42a0b088d9468c972193",
            "value": 570
          }
        },
        "96bab606dfc9471f9bc00281640e50f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7abb76323d4845a3ac9fa3aa580b6a19",
            "placeholder": "​",
            "style": "IPY_MODEL_0a837a414f554df8b7ee3093f8954bf8",
            "value": " 570/570 [00:00&lt;00:00, 28.7kB/s]"
          }
        },
        "ccb2770088834a7ebf9f0e1859f19827": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ab215df3a5c48cba8b438b993980610": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d9ada0bbaa24935bb3956e6f8316b18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08043920db32475ca938101666cd1600": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5843f402223f42a0b088d9468c972193": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7abb76323d4845a3ac9fa3aa580b6a19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a837a414f554df8b7ee3093f8954bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sukriti-Narang/Hate-Speech-Analysis/blob/main/MAJOR_PROJECT_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NbVvmPICEx5"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import os\n",
        "\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CLEAN_TWEETS = False\n"
      ],
      "metadata": {
        "id": "pJNZ50BED69f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, re\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "V90JGp3RD-Du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "print(device_name)"
      ],
      "metadata": {
        "id": "Qw6RcKQKECNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kek38gNpMSJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/labeled_data.csv')\n",
        "\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "metadata": {
        "id": "xh_tZsa9J70V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#strip tweet related character\n",
        "def strip_all_entities(x):\n",
        "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x).split())\n",
        "#check functionality\n",
        "print(df['tweet'][1])\n",
        "strip_all_entities(df['tweet'][1])"
      ],
      "metadata": {
        "id": "LqWNbrNAJHAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# .apply() function  help in applying \"strip_all_entities\" across all the tweet\n",
        "if CLEAN_TWEETS == False:\n",
        "    df['tweet']=df['tweet'].apply(strip_all_entities)\n"
      ],
      "metadata": {
        "id": "_BGiq3ieJHCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/labeled_data.csv')\n",
        "\n",
        "# Data Preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'@\\S+', '', text)\n",
        "    text = re.sub(r'#\\S+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "# Apply data preprocessing to the 'comment' column\n",
        "df['tweet'] = df['tweet'].apply(preprocess_text)\n",
        "# Unnamed: 0\tcount\thate_speech\toffensive_language\tneither\tclass"
      ],
      "metadata": {
        "id": "YHK8_CNK05GC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = df['tweet'].values\n",
        "# Sentence = Array of all preprocessed tweets\n",
        "print(sentences)\n",
        "# Label = Array of all class\n",
        "labels = df['class'].values\n",
        "print(labels)"
      ],
      "metadata": {
        "id": "ObIYv5L1JHEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split into train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df, random_state=1508, shuffle=True, test_size=0.2)"
      ],
      "metadata": {
        "id": "t16zHFILJHHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Further split test into test and validate\n",
        "#Validate is usually used for hyperparameter tuning\n",
        "test, validation = train_test_split(test, random_state=1508, shuffle=True, test_size=0.5)\n"
      ],
      "metadata": {
        "id": "xHuv6kmCJHJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "trn_sentences = train['tweet'].values\n",
        "train_labels = train['class'].values\n",
        "\n",
        "tst_sentences = test['tweet'].values\n",
        "test_labels = test['class'].values\n",
        "\n",
        "val_sentences = validation['tweet'].values\n",
        "validation_labels = validation['class'].values"
      ],
      "metadata": {
        "id": "D4VlWW4wJHL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "fGtbNSoHUWS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install catboost\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzhRZoJGkrQX",
        "outputId": "adb25692-6473-48f2-9699-3abafed5c746"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.10/dist-packages (1.2.5)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.11.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Ensure NLTK stopwords are downloaded\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('labeled_data.csv')\n",
        "\n",
        "# Drop the 'Unnamed: 0' column\n",
        "data = data.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "# Text cleaning function\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'@\\w+', '', text)     # Remove mentions\n",
        "    text = re.sub(r'#\\w+', '', text)     # Remove hashtags\n",
        "    text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = text.lower()                  # Lowercase text\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "# Apply text cleaning\n",
        "data['tweet'] = data['tweet'].apply(clean_text)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data['tweet']\n",
        "y = data['class']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize the 'tweet' column using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=10000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Manually selected hyperparameters\n",
        "best_params = {\n",
        "    'iterations': 1500,       # Increase iterations for better convergence\n",
        "    'learning_rate': 0.1,     # Typical learning rate\n",
        "    'depth': 8,               # Medium depth\n",
        "    'l2_leaf_reg': 5          # Regularization strength\n",
        "}\n",
        "\n",
        "# Initialize the CatBoost classifier with manually selected hyperparameters\n",
        "best_model = CatBoostClassifier(**best_params, verbose=100)\n",
        "best_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = best_model.predict(X_test_tfidf)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Tx06wRacGGw",
        "outputId": "faea6647-75d1-4876-a825-d4a024035350"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\tlearn: 0.9944214\ttotal: 1.79s\tremaining: 44m 38s\n",
            "100:\tlearn: 0.3356714\ttotal: 2m 13s\tremaining: 30m 49s\n",
            "200:\tlearn: 0.3084054\ttotal: 4m 18s\tremaining: 27m 50s\n",
            "300:\tlearn: 0.2992978\ttotal: 6m 19s\tremaining: 25m 12s\n",
            "400:\tlearn: 0.2925664\ttotal: 8m 25s\tremaining: 23m 4s\n",
            "500:\tlearn: 0.2868566\ttotal: 10m 32s\tremaining: 21m\n",
            "600:\tlearn: 0.2832435\ttotal: 12m 34s\tremaining: 18m 48s\n",
            "700:\tlearn: 0.2795625\ttotal: 14m 40s\tremaining: 16m 43s\n",
            "800:\tlearn: 0.2756029\ttotal: 16m 45s\tremaining: 14m 37s\n",
            "900:\tlearn: 0.2726879\ttotal: 18m 50s\tremaining: 12m 31s\n",
            "1000:\tlearn: 0.2691982\ttotal: 20m 55s\tremaining: 10m 25s\n",
            "1100:\tlearn: 0.2665896\ttotal: 23m\tremaining: 8m 20s\n",
            "1200:\tlearn: 0.2633522\ttotal: 25m 8s\tremaining: 6m 15s\n",
            "1300:\tlearn: 0.2607317\ttotal: 27m 17s\tremaining: 4m 10s\n",
            "1400:\tlearn: 0.2580416\ttotal: 29m 22s\tremaining: 2m 4s\n",
            "1499:\tlearn: 0.2558123\ttotal: 31m 28s\tremaining: 0us\n",
            "Best Parameters: {'iterations': 1500, 'learning_rate': 0.1, 'depth': 8, 'l2_leaf_reg': 5}\n",
            "Accuracy: 0.8961065160379261\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.18      0.26       290\n",
            "           1       0.94      0.94      0.94      3832\n",
            "           2       0.78      0.94      0.85       835\n",
            "\n",
            "    accuracy                           0.90      4957\n",
            "   macro avg       0.74      0.69      0.68      4957\n",
            "weighted avg       0.88      0.90      0.88      4957\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Ensure NLTK stopwords are downloaded\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('labeled_data.csv')\n",
        "\n",
        "# Drop the 'Unnamed: 0' column\n",
        "data = data.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "# Text cleaning function\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'@\\w+', '', text)     # Remove mentions\n",
        "    text = re.sub(r'#\\w+', '', text)     # Remove hashtags\n",
        "    text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = text.lower()                  # Lowercase text\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "# Apply text cleaning\n",
        "data['tweet'] = data['tweet'].apply(clean_text)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data['tweet']\n",
        "y = data['class']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize the 'tweet' column using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=10000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "best_accuracy = 0\n",
        "best_params = {}\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'iterations': [100, 200, 300],\n",
        "    'learning_rate': [0.05],\n",
        "    'depth': 8,               # Medium depth\n",
        "    'l2_leaf_reg': 5          # Regularization strength\n",
        "}\n",
        "\n",
        "# Iterate over combinations of hyperparameters\n",
        "for iterations in param_grid['iterations']:\n",
        "    for learning_rate in param_grid['learning_rate']:\n",
        "        # Initialize the CatBoost classifier with the current set of hyperparameters\n",
        "        catboost_clf = CatBoostClassifier(iterations=iterations,\n",
        "                                          learning_rate=learning_rate,\n",
        "                                          verbose=0)\n",
        "        # Train the classifier\n",
        "        catboost_clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "        # Evaluate the classifier on the test set\n",
        "        y_pred = catboost_clf.predict(X_test_tfidf)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        # Update the best accuracy and best parameters if the current model is better\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_params = {'iterations': iterations,\n",
        "                           'learning_rate': learning_rate}\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Accuracy:\", best_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buVtMgnp4OvF",
        "outputId": "47c73b0a-4e5d-4887-9cb4-35e6e6cacafd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'iterations': 300, 'learning_rate': 0.05}\n",
            "Best Accuracy: 0.8690740367157554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.ensemble import BaggingClassifier\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# # Splitting the dataset into train and test sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(df['tweet'], df['class'], test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# # Convert text data into numerical features using CountVectorizer\n",
        "# vectorizer = CountVectorizer()\n",
        "# X_train_vec = vectorizer.fit_transform(X_train)\n",
        "# X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "\n",
        "# # Initialize base classifier\n",
        "# base_classifier = DecisionTreeClassifier()\n",
        "\n",
        "# # Initialize BaggingClassifier\n",
        "# bagging_clf = BaggingClassifier(base_estimator=base_classifier, n_estimators=10, random_state=42)\n",
        "\n",
        "# # Train BaggingClassifier\n",
        "# bagging_clf.fit(X_train_vec, y_train)\n",
        "\n",
        "# # Evaluate classifier on test set\n",
        "# bagging_predictions = bagging_clf.predict(X_test_vec)\n",
        "\n",
        "# # Print classification report\n",
        "# print(\"Bagging Classifier Report:\")\n",
        "# print(classification_report(y_test, bagging_predictions))\n"
      ],
      "metadata": {
        "id": "kP4Lo18PUD-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Splitting the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['tweet'], df['class'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data into TF-IDF features\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize base classifier\n",
        "base_classifier = DecisionTreeClassifier()\n",
        "\n",
        "# Initialize BaggingClassifier\n",
        "bagging_clf = BaggingClassifier(base_estimator=base_classifier, n_estimators=10, random_state=42)\n",
        "\n",
        "# Train BaggingClassifier\n",
        "bagging_clf.fit(X_train_vec, y_train)\n",
        "\n",
        "# Evaluate classifier on test set\n",
        "bagging_predictions = bagging_clf.predict(X_test_vec)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Bagging Classifier Report:\")\n",
        "print(classification_report(y_test, bagging_predictions))\n",
        "\n",
        "# DataFrame containing hate tweets\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame with the test tweets and their corresponding predicted classes\n",
        "test_results_df = pd.DataFrame({'tweet': X_test, 'predicted_class': bagging_predictions, 'actual_class': y_test})\n",
        "\n",
        "# Filter the DataFrame to include only tweets where the predicted class is 2\n",
        "class_2_tweets_df = test_results_df[test_results_df['predicted_class'] == 2]\n",
        "\n",
        "# Print the DataFrame containing class 2 tweets\n",
        "print(\"DataFrame of tweets where predicted class is 2:\")\n",
        "print(class_2_tweets_df)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DyrcwIRdNdM",
        "outputId": "21948084-eeed-4724-e1d2-0cbe857325ff"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.22      0.27       290\n",
            "           1       0.93      0.94      0.93      3832\n",
            "           2       0.81      0.88      0.85       835\n",
            "\n",
            "    accuracy                           0.89      4957\n",
            "   macro avg       0.70      0.68      0.68      4957\n",
            "weighted avg       0.88      0.89      0.88      4957\n",
            "\n",
            "DataFrame of tweets where predicted class is 2:\n",
            "                                                   tweet  predicted_class  \\\n",
            "4273        what did you search gay redneck episode play                2   \n",
            "3778   gotta love it when the islamofascist cow tries...                2   \n",
            "4794                    their tortillas are trash though                2   \n",
            "15789  rt jsu coach omar johnson its what u do agains...                2   \n",
            "11311  im just tryna get to sleep before the birds st...                2   \n",
            "...                                                  ...              ...   \n",
            "697    sniffs whiffy balls involuntary cuz a fairy wa...                2   \n",
            "10959     i think ill just eat this brownie and pass out                2   \n",
            "20979                             so real its unreal lol                2   \n",
            "7339                                      uh youre trash                2   \n",
            "20769  she unfollowed me after i said i cried watchin...                2   \n",
            "\n",
            "       actual_class  \n",
            "4273              0  \n",
            "3778              0  \n",
            "4794              2  \n",
            "15789             2  \n",
            "11311             2  \n",
            "...             ...  \n",
            "697               2  \n",
            "10959             2  \n",
            "20979             2  \n",
            "7339              1  \n",
            "20769             2  \n",
            "\n",
            "[910 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Function to perform sentiment analysis on a text\n",
        "def analyze_sentiment(text):\n",
        "    # Create a TextBlob object\n",
        "    blob = TextBlob(text)\n",
        "\n",
        "    # Get the polarity score (-1 to 1, where -1 is very negative, 0 is neutral, and 1 is very positive)\n",
        "    polarity = blob.sentiment.polarity\n",
        "\n",
        "    # Classify sentiment based on polarity score\n",
        "    if polarity < -0.5:\n",
        "        return 'high'\n",
        "    elif -0.5 <= polarity < 0:\n",
        "        return 'mild'\n",
        "    elif polarity == 0:\n",
        "        return 'low'\n",
        "    elif 0 < polarity <= 0.5:\n",
        "        return 'low'\n",
        "    else:\n",
        "        return 'low'\n",
        "\n",
        "# Create a copy of the DataFrame\n",
        "class_2_tweets_df_copy = class_2_tweets_df.copy()\n",
        "\n",
        "# Apply sentiment analysis to each tweet in the copied DataFrame\n",
        "class_2_tweets_df_copy['sentiment'] = class_2_tweets_df_copy['tweet'].apply(analyze_sentiment)\n",
        "\n",
        "# Print the DataFrame with sentiment analysis results\n",
        "print(\"DataFrame with sentiment analysis results:\")\n",
        "print(class_2_tweets_df_copy)\n",
        "\n",
        "\n",
        "# Print the DataFrame with sentiment analysis results\n",
        "print(\"DataFrame with sentiment analysis results:\")\n",
        "print(class_2_tweets_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5K8Jtj44tR27",
        "outputId": "9a8520b7-9236-4c33-c37d-e126f8da6901"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame with sentiment analysis results:\n",
            "                                                   tweet  predicted_class  \\\n",
            "4273        what did you search gay redneck episode play                2   \n",
            "3778   gotta love it when the islamofascist cow tries...                2   \n",
            "4794                    their tortillas are trash though                2   \n",
            "15789  rt jsu coach omar johnson its what u do agains...                2   \n",
            "11311  im just tryna get to sleep before the birds st...                2   \n",
            "...                                                  ...              ...   \n",
            "697    sniffs whiffy balls involuntary cuz a fairy wa...                2   \n",
            "10959     i think ill just eat this brownie and pass out                2   \n",
            "20979                             so real its unreal lol                2   \n",
            "7339                                      uh youre trash                2   \n",
            "20769  she unfollowed me after i said i cried watchin...                2   \n",
            "\n",
            "       actual_class sentiment  \n",
            "4273              0       low  \n",
            "3778              0      mild  \n",
            "4794              2       low  \n",
            "15789             2       low  \n",
            "11311             2       low  \n",
            "...             ...       ...  \n",
            "697               2      mild  \n",
            "10959             2      mild  \n",
            "20979             2       low  \n",
            "7339              1       low  \n",
            "20769             2       low  \n",
            "\n",
            "[910 rows x 4 columns]\n",
            "DataFrame with sentiment analysis results:\n",
            "                                                   tweet  predicted_class  \\\n",
            "4273        what did you search gay redneck episode play                2   \n",
            "3778   gotta love it when the islamofascist cow tries...                2   \n",
            "4794                    their tortillas are trash though                2   \n",
            "15789  rt jsu coach omar johnson its what u do agains...                2   \n",
            "11311  im just tryna get to sleep before the birds st...                2   \n",
            "...                                                  ...              ...   \n",
            "697    sniffs whiffy balls involuntary cuz a fairy wa...                2   \n",
            "10959     i think ill just eat this brownie and pass out                2   \n",
            "20979                             so real its unreal lol                2   \n",
            "7339                                      uh youre trash                2   \n",
            "20769  she unfollowed me after i said i cried watchin...                2   \n",
            "\n",
            "       actual_class  \n",
            "4273              0  \n",
            "3778              0  \n",
            "4794              2  \n",
            "15789             2  \n",
            "11311             2  \n",
            "...             ...  \n",
            "697               2  \n",
            "10959             2  \n",
            "20979             2  \n",
            "7339              1  \n",
            "20769             2  \n",
            "\n",
            "[910 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of tweets in each sentiment category\n",
        "sentiment_counts = class_2_tweets_df_copy['sentiment'].value_counts()\n",
        "\n",
        "# Calculate the total number of tweets\n",
        "total_tweets = len(class_2_tweets_df_copy)\n",
        "\n",
        "# Calculate the percentage of tweets in each sentiment category\n",
        "high_hate_percentage = (sentiment_counts.get('high', 0) / total_tweets) * 100\n",
        "mild_hate_percentage = (sentiment_counts.get('mild', 0) / total_tweets) * 100\n",
        "low_hate_percentage = (sentiment_counts.get('low', 0) / total_tweets) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(\"Percentage of tweets classified as high hate:\", high_hate_percentage)\n",
        "print(\"Percentage of tweets classified as mild hate:\", mild_hate_percentage)\n",
        "print(\"Percentage of tweets classified as low hate:\", low_hate_percentage)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kC2hLwducoy",
        "outputId": "e64a42c0-97b3-4428-8e9a-2784e8a45731"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of tweets classified as high hate: 2.307692307692308\n",
            "Percentage of tweets classified as mild hate: 18.461538461538463\n",
            "Percentage of tweets classified as low hate: 79.23076923076923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initialize SVC\n",
        "svm_clf = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "\n",
        "# Train SVM\n",
        "svm_clf.fit(X_train_vec, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "svm_predictions = svm_clf.predict(X_test_vec)\n",
        "\n",
        "# Print classification report\n",
        "print(\"SVM Classifier Report:\")\n",
        "print(classification_report(y_test, svm_predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtIjz3byUEBQ",
        "outputId": "32fc4027-7a58-4625-c66e-73e0b7f262ed"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Classifier Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.47      0.13      0.20       290\n",
            "           1       0.92      0.96      0.94      3832\n",
            "           2       0.83      0.88      0.85       835\n",
            "\n",
            "    accuracy                           0.90      4957\n",
            "   macro avg       0.74      0.66      0.66      4957\n",
            "weighted avg       0.88      0.90      0.88      4957\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dtNt_J7tsrqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "enOr-vJL6nZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nhEkj_2v6nbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8SLuUgXJ6ndq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bs47O9vr6ngE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jC484gH66nin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d42CdCsR6nmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Initialize base classifiers\n",
        "base_classifiers = [\n",
        "    ('bagging', BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)),\n",
        "    ('svm', SVC(kernel='linear', C=1.0, random_state=42))\n",
        "]\n",
        "\n",
        "# Initialize VotingClassifier\n",
        "voting_clf = VotingClassifier(estimators=base_classifiers, voting='hard')\n",
        "\n",
        "# Train VotingClassifier\n",
        "voting_clf.fit(X_train_vec, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "voting_predictions = voting_clf.predict(X_test_vec)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Voting Ensemble Classifier Report:\")\n",
        "print(classification_report(y_test, voting_predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfDOAK4rUED-",
        "outputId": "43f416c3-b56e-42bc-a365-728fca9f354e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voting Ensemble Classifier Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      0.27      0.31       290\n",
            "           1       0.92      0.94      0.93      3832\n",
            "           2       0.85      0.83      0.84       835\n",
            "\n",
            "    accuracy                           0.89      4957\n",
            "   macro avg       0.71      0.68      0.69      4957\n",
            "weighted avg       0.88      0.89      0.88      4957\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Nt3dBrPUEHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "print('loading Bert tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321,
          "referenced_widgets": [
            "01f2d5d3e38c4aabaa8496b43ec4264a",
            "9104a65b05e14d66b49e60cd558c92f1",
            "edf0d7a544274c55bb2347ec509b5a9b",
            "a34f690719b54de6b27f2d293c853cc9",
            "7872c0067bf44c0db79b3b0f663af5ef",
            "51fa495a46a049679d9eecc9ac00cfe7",
            "9079f7ad68d444aabfcf2527d611ecb4",
            "1d3410e6b16f4c5697e6c13c2496a36f",
            "ab666f866eb241a5b0d9898143f37551",
            "cb7f590842874b57be6602f106a79365",
            "f3f6e78931534089bc64490833d071ac",
            "d1dd3fef1c26460abb1a99855068155e",
            "b1e3791d3a094fd480d7955ae1b9369f",
            "71405156e1904ebe892a49ab87e5bb87",
            "e16fed2f40fe4bcb98af886f67076961",
            "ba3e1bf734f84b3f8de22a80f47d4dca",
            "0eb759e5f6b14045afce5f08d689f6b5",
            "b947ec9561004cb0b396ce2a598d1621",
            "f3ffb39c30de4ce781e2671fc9ddfaab",
            "9fa4d6c0d5df43c4acc577c4a5c48aed",
            "8325d6409b26494d8a4c9bcabbeb3518",
            "04fa6f6e96e3459c8d36b7919a21984f",
            "ce7e169a4bb04ed7885449c8de213f0c",
            "5be02c2aa70f41f88e5ee5d32cf2c45d",
            "9483c7ce076540b384e5bf7e4219d107",
            "a412ed9c10884f4caa5c4394111dfc40",
            "f1ec8033c2c548b49c68a2a8716b24ed",
            "7b9e5c8f18244a5491b6ab192dfd8a56",
            "297557a65b9a42fc8686186311dc8cfc",
            "03d8723f30d64f1cbffac52e706d89d7",
            "1cc2b2012d0a4b5d94bd23ba970fc94f",
            "19a41dd6327341169ed4b6a301d6c585",
            "cdccb6d2aa0a4f7d8b5d4dcdbc0a6e43",
            "4b9f6f5cab044cddadf467eaca2b3d6f",
            "c3ca284e91cf4034a4d8ca93edb09675",
            "a76ebbe6634d459ab31b1e70d5b63bd5",
            "96bab606dfc9471f9bc00281640e50f9",
            "ccb2770088834a7ebf9f0e1859f19827",
            "6ab215df3a5c48cba8b438b993980610",
            "9d9ada0bbaa24935bb3956e6f8316b18",
            "08043920db32475ca938101666cd1600",
            "5843f402223f42a0b088d9468c972193",
            "7abb76323d4845a3ac9fa3aa580b6a19",
            "0a837a414f554df8b7ee3093f8954bf8"
          ]
        },
        "id": "AW1XRyO6JHN4",
        "outputId": "2a14761a-f2f9-45e8-d33e-63db532878fc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Bert tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "01f2d5d3e38c4aabaa8496b43ec4264a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1dd3fef1c26460abb1a99855068155e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce7e169a4bb04ed7885449c8de213f0c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b9f6f5cab044cddadf467eaca2b3d6f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_exp = tokenizer.encode('You are lame')\n",
        "dec_exp = tokenizer.decode(enc_exp)\n",
        "print(enc_exp)\n",
        "print(dec_exp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pr-_WUTUJHQ7",
        "outputId": "4398f6b9-45db-405f-b4ed-e7fb3ea0d204"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 2017, 2024, 20342, 102]\n",
            "[CLS] you are lame [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Maximum length in a sentence\n",
        "print('Max array length: ', max([len(i) for i in sentences]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqpjidKGVtWo",
        "outputId": "11ef9b75-3fc5-456e-9282-04957c01c29d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max array length:  141\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count1, count2, count3 = 0, 0, 0\n",
        "for i in sentences:\n",
        "    if len(i)>100:\n",
        "        count1+=1\n",
        "    if len(i)>200:\n",
        "        count2+=1\n",
        "    if len(i)>300:\n",
        "        count3+=1\n",
        "print('number of array longer than 100: ', count1)\n",
        "print('number of array longer than 200: ', count2)\n",
        "print('number of array longer than 300: ', count3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9HOxdiMVtY1",
        "outputId": "670bfcf2-ed9b-4aa0-ebeb-5d81cad9533f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of array longer than 100:  4141\n",
            "number of array longer than 200:  0\n",
            "number of array longer than 300:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install catboost\n"
      ],
      "metadata": {
        "id": "RSMAUdqqXlP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('labeled_data.csv')\n",
        "\n",
        "# Drop the 'Unnamed: 0' column\n",
        "data = data.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data['tweet']\n",
        "y = data['class']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize the 'tweet' column using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=10000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train a CatBoost classifier\n",
        "model = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, verbose=100)\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yggk0MpxVtbA",
        "outputId": "4c8745ce-c21f-4fab-9560-254a30d408b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\tlearn: 0.9945848\ttotal: 1.49s\tremaining: 24m 52s\n",
            "100:\tlearn: 0.3338495\ttotal: 47.9s\tremaining: 7m 6s\n",
            "200:\tlearn: 0.3038636\ttotal: 1m 31s\tremaining: 6m 4s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_eKlx5UxftCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from scipy.stats import randint\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('labeled_data.csv')\n",
        "\n",
        "# Drop the 'Unnamed: 0' column\n",
        "data = data.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "# Text cleaning function\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'@\\w+', '', text)     # Remove mentions\n",
        "    text = re.sub(r'#\\w+', '', text)     # Remove hashtags\n",
        "    text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = text.lower()                  # Lowercase text\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "# Apply text cleaning\n",
        "data['tweet'] = data['tweet'].apply(clean_text)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data['tweet']\n",
        "y = data['class']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize the 'tweet' column using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=10000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_dist = {\n",
        "    'iterations': randint(500, 2000),\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'depth': randint(4, 12),\n",
        "    'l2_leaf_reg': randint(1, 10)\n",
        "}\n",
        "\n",
        "# Initialize the CatBoost classifier\n",
        "catboost_clf = CatBoostClassifier(verbose=0)\n",
        "\n",
        "# Set up RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=catboost_clf, param_distributions=param_dist, n_iter=100, cv=5, scoring='accuracy', verbose=3, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Perform the random search\n",
        "random_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = random_search.best_params_\n",
        "\n",
        "# Train a CatBoost classifier with the best parameters\n",
        "best_model = CatBoostClassifier(**best_params, verbose=100)\n",
        "best_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = best_model.predict(X_test_tfidf)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", report)\n"
      ],
      "metadata": {
        "id": "pRiOjrY3gAXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Define hyperparameters for tuning\n",
        "depth = [4, 6, 8]  # Depth of the trees\n",
        "learning_rate = [0.01, 0.05, 0.1]  # Learning rate\n",
        "l2_leaf_reg = [1, 3, 5]  # L2 regularization coefficient\n",
        "iterations = [100, 200, 300]  # Number of iterations\n",
        "\n",
        "best_score = 0\n",
        "best_params = {}\n",
        "\n",
        "# Iterate over all combinations of hyperparameters\n",
        "for d in depth:\n",
        "    for lr in learning_rate:\n",
        "        for l2 in l2_leaf_reg:\n",
        "            for it in iterations:\n",
        "                # Initialize CatBoostClassifier with current hyperparameters\n",
        "                catboost_clf = CatBoostClassifier(depth=d, learning_rate=lr, l2_leaf_reg=l2, iterations=it)\n",
        "\n",
        "                # Train CatBoostClassifier\n",
        "                catboost_clf.fit(X_train_vec, y_train)\n",
        "\n",
        "                # Evaluate classifier on test set\n",
        "                catboost_predictions = catboost_clf.predict(X_test_vec)\n",
        "\n",
        "                # Calculate classification report\n",
        "                report = classification_report(y_test, catboost_predictions, output_dict=True)\n",
        "                f1_score = report['weighted avg']['f1-score']\n",
        "\n",
        "                # Check if current hyperparameters yield a better F1 score\n",
        "                if f1_score > best_score:\n",
        "                    best_score = f1_score\n",
        "                    best_params = {'depth': d, 'learning_rate': lr, 'l2_leaf_reg': l2, 'iterations': it}\n",
        "\n",
        "# Train CatBoostClassifier with the best hyperparameters\n",
        "best_catboost_clf = CatBoostClassifier(**best_params)\n",
        "best_catboost_clf.fit(X_train_vec, y_train)\n",
        "\n",
        "# Evaluate classifier on test set\n",
        "best_catboost_predictions = best_catboost_clf.predict(X_test_vec)\n",
        "\n",
        "# Print classification report for the best model\n",
        "print(\"Best CatBoost Classifier Report:\")\n",
        "print(classification_report(y_test, best_catboost_predictions))\n"
      ],
      "metadata": {
        "id": "dvVgjJjc_0mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Load your dataset\n",
        "# Replace 'your_dataset.csv' with the path to your dataset\n",
        "df = pd.read_csv('/content/labeled_data.csv')\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['tweet'], df['class'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data into numerical features using CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_distributions = {\n",
        "    'depth': [4, 6, 8, 10],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'l2_leaf_reg': [1, 3, 5],\n",
        "    'iterations': [100, 200, 300]\n",
        "}\n",
        "\n",
        "# Initialize CatBoostClassifier\n",
        "catboost_clf = CatBoostClassifier()\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(catboost_clf, param_distributions, n_iter=10, scoring='f1_weighted', cv=5, verbose=2, n_jobs=1)\n",
        "\n",
        "# Perform random search for hyperparameter tuning\n",
        "random_search.fit(X_train_vec, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = random_search.best_params_\n",
        "\n",
        "# Train CatBoostClassifier with the best hyperparameters\n",
        "best_catboost_clf = CatBoostClassifier(**best_params)\n",
        "best_catboost_clf.fit(X_train_vec, y_train)\n",
        "\n",
        "# Evaluate classifier on test set\n",
        "best_catboost_predictions = best_catboost_clf.predict(X_test_vec)\n",
        "\n",
        "# Print classification report for the best model\n",
        "print(\"Best CatBoost Classifier Report:\")\n",
        "print(classification_report(y_test, best_catboost_predictions))\n"
      ],
      "metadata": {
        "id": "SAvHf7S_CF7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H2elC7eLEj73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Splitting the dataset into train and test sets\n",
        "try:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df['tweet'], df['class'], test_size=0.2, random_state=42)\n",
        "\n",
        "    # Convert text data into numerical features using CountVectorizer\n",
        "    vectorizer = CountVectorizer()\n",
        "    X_train_vec = vectorizer.fit_transform(X_train)\n",
        "    X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "    # Initialize CatBoostClassifier\n",
        "    catboost_clf = CatBoostClassifier()\n",
        "\n",
        "    # Train CatBoostClassifier\n",
        "    catboost_clf.fit(X_train_vec, y_train)\n",
        "\n",
        "    # Evaluate classifier on test set\n",
        "    catboost_predictions = catboost_clf.predict(X_test_vec)\n",
        "\n",
        "    # Print classification report\n",
        "    print(\"CatBoost Classifier Report:\")\n",
        "    print(classification_report(y_test, catboost_predictions))\n",
        "\n",
        "except TypeError as e:\n",
        "    print(\"An error occurred while training the CatBoostClassifier:\", e)\n",
        "    # Handle the error gracefully, e.g., by skipping problematic data points or applying data imputation techniques\n"
      ],
      "metadata": {
        "id": "DcwQEySi2H4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Initialize the sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Analyze the sentiment of each tweet in the test set\n",
        "sentiments = [sia.polarity_scores(tweet)['compound'] for tweet in X_test]\n",
        "\n",
        "# Classify sentiments as positive, negative, or neutral based on compound score\n",
        "sentiment_labels = ['positive' if score > 0.05 else 'negative' if score < -0.05 else 'neutral' for score in sentiments]\n",
        "# Convert sentiment labels to numeric values\n",
        "sentiment_mapping = {'positive': 1, 'negative': 0, 'neutral': 2}\n",
        "sentiment_labels_numeric = [sentiment_mapping[label] for label in sentiment_labels]\n",
        "\n",
        "# Print sentiment classification report\n",
        "print(\"Sentiment Analysis Report:\")\n",
        "print(classification_report(y_test, sentiment_labels_numeric))\n",
        "\n"
      ],
      "metadata": {
        "id": "NRSoRk9lzXx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count the occurrences of each sentiment label\n",
        "sentiment_counts = {'negative': 0, 'positive': 0, 'neutral': 0}\n",
        "for label in sentiment_labels:\n",
        "    sentiment_counts[label] += 1\n",
        "\n",
        "# Plot the bar graph\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(sentiment_counts.keys(), sentiment_counts.values(), color=['red', 'green', 'blue'])\n",
        "plt.title('Distribution of Sentiment Labels')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BnpNlJMsz781"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CTfkQ_SD_bRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3zfLRLjg_bfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "id": "dr_2oiezzlZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Splitting the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['tweet'], df['class'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data into numerical features using CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Define the objective function to optimize\n",
        "def objective_function(learning_rate, depth, l2_leaf_reg):\n",
        "    # Convert hyperparameters to appropriate types\n",
        "    learning_rate = float(learning_rate)\n",
        "    depth = int(depth)\n",
        "    l2_leaf_reg = int(l2_leaf_reg)\n",
        "\n",
        "    # Define CatBoostClassifier with hyperparameters\n",
        "    clf = CatBoostClassifier(learning_rate=learning_rate, depth=depth, l2_leaf_reg=l2_leaf_reg)\n",
        "\n",
        "    # Evaluate classifier using cross-validation\n",
        "    scores = cross_val_score(clf, X_train_vec[:1000], y_train[:1000], cv=5, scoring='accuracy')  # Using a subset of the data\n",
        "\n",
        "    # Return the mean cross-validation score\n",
        "    return scores.mean()\n",
        "\n",
        "# Define the search space for hyperparameters\n",
        "pbounds = {'learning_rate': (0.01, 0.1),\n",
        "           'depth': (4, 8),\n",
        "           'l2_leaf_reg': (1, 5)}\n",
        "\n",
        "# Initialize BayesianOptimization object with objective function and search space\n",
        "optimizer = BayesianOptimization(f=objective_function, pbounds=pbounds, random_state=42)\n",
        "\n",
        "# Perform optimization\n",
        "optimizer.maximize(init_points=5, n_iter=10)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = optimizer.max['params']\n",
        "print(\"Best Hyperparameters:\", best_params)\n"
      ],
      "metadata": {
        "id": "gIDRoATjtZWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bayesian-optimization\n"
      ],
      "metadata": {
        "id": "kBzGmiNMv1UF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "# Define the parameter grid for CatBoostClassifier\n",
        "param_grid = {\n",
        "    'iterations': [100, 150, 200],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'depth': [4, 6, 8],\n",
        "    'l2_leaf_reg': [1, 3, 5]\n",
        "}\n",
        "\n",
        "# Splitting the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['tweet'], df['class'], test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Convert text data into numerical features using CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize CatBoostClassifier\n",
        "catboost_clf = CatBoostClassifier(iterations=1000, early_stopping_rounds=10)\n",
        "\n",
        "\n",
        "# Initialize GridSearchCV with early stopping\n",
        "grid_search = GridSearchCV(estimator=catboost_clf, param_grid=param_grid, cv=5, n_jobs=-1,\n",
        "                           scoring='accuracy', verbose=2)\n",
        "\n",
        "# Fit the grid search to the data with early stopping\n",
        "grid_search.fit(X_train_vec, y_train,\n",
        "                catboost_clf__early_stopping_rounds=10,  # Number of rounds to wait for early stopping\n",
        "                catboost_clf__eval_set=(X_test_vec, y_test),  # Evaluation set for early stopping\n",
        "                catboost_clf__verbose=10)  # Print evaluation metrics for each round\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Get the best model\n",
        "best_catboost = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on test set\n",
        "best_catboost_predictions = best_catboost.predict(X_test_vec)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Best CatBoost Classifier Report:\")\n",
        "print(classification_report(y_test, best_catboost_predictions))\n"
      ],
      "metadata": {
        "id": "7muzOnnqhUPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# Initialize CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "\n",
        "# Splitting the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['tweet'], df['class'], test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Convert text data into numerical features\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize AdaBoostClassifier\n",
        "adaboost_clf = AdaBoostClassifier()\n",
        "\n",
        "# Train AdaBoostClassifier\n",
        "adaboost_clf.fit(X_train_vec, y_train)\n",
        "\n",
        "# Evaluate classifier on test set\n",
        "adaboost_predictions = adaboost_clf.predict(X_test_vec)\n",
        "\n",
        "# Print classification report\n",
        "print(\"AdaBoost Classifier Report:\")\n",
        "print(classification_report(y_test, adaboost_predictions))\n"
      ],
      "metadata": {
        "id": "Hmas0r84VtdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the hyperparameters grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.1, 0.5, 1.0]\n",
        "}\n",
        "\n",
        "# Initialize AdaBoostClassifier\n",
        "adaboost_clf = AdaBoostClassifier()\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(adaboost_clf, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Perform hyperparameter tuning\n",
        "grid_search.fit(X_train_vec, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best hyperparameters:\", best_params)\n",
        "\n",
        "# Evaluate the classifier with the best hyperparameters\n",
        "best_adaboost_clf = grid_search.best_estimator_\n",
        "best_adaboost_predictions = best_adaboost_clf.predict(X_test_vec)\n",
        "\n",
        "# Print classification report\n",
        "print(\"AdaBoost Classifier Report:\")\n",
        "print(classification_report(y_test, best_adaboost_predictions))\n"
      ],
      "metadata": {
        "id": "ulpAqmk2rBGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_test, adaboost_predictions)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', cbar=False)\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix - AdaBoost Classifier')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oxwih7cDqREF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Splitting the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['tweet'], df['class'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data into numerical features using CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize Random Forest Classifier\n",
        "random_forest_clf = RandomForestClassifier()\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "random_forest_clf.fit(X_train_vec, y_train)\n",
        "\n",
        "# Evaluate classifier on test set\n",
        "random_forest_predictions = random_forest_clf.predict(X_test_vec)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Random Forest Classifier Report:\")\n",
        "print(classification_report(y_test, random_forest_predictions))\n",
        "\n"
      ],
      "metadata": {
        "id": "m1J5S1NAVtgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SMXQbSmRdHpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install catboost\n",
        "\n"
      ],
      "metadata": {
        "id": "cXMVYESXdH_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/labeled_data.csv')\n",
        "\n",
        "# Data Preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'@\\S+', '', text)\n",
        "    text = re.sub(r'#\\S+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "# Apply data preprocessing to the 'comment' column\n",
        "df['tweet'] = df['tweet'].apply(preprocess_text)\n",
        "# Unnamed: 0\tcount\thate_speech\toffensive_language\tneither\tclass\n",
        "\n",
        "# threshold = -5.0\n",
        "# df['hate_speech_score'] = (df['hate_speech_score'] >= threshold).astype(int)\n",
        "\n",
        "# # Split the dataset into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(df['text'], df['hate_speech_score'], test_size=0.2, random_state=14)\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['tweet'], df['class'], test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Create a TF-IDF vectorizer to convert text data into numerical features\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust the max_features as needed\n",
        "\n",
        "# Fit and transform the vectorizer on the training data\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Save the TF-IDF vectorizer to a file using pickle\n",
        "with open('tfidf_vectorizer.pkl', 'wb') as file:\n",
        "    pickle.dump(tfidf_vectorizer, file)\n",
        "\n",
        "# Perform manual hyperparameter tuning\n",
        "best_accuracy = 0\n",
        "best_hyperparameters = {}\n",
        "for depth in [4, 6, 8, 10]:\n",
        "    for iterations in [50, 100, 150]:\n",
        "        # Create and train a CatBoost classifier\n",
        "        clf = CatBoostClassifier(depth=depth, iterations=iterations, verbose=0, random_state=42)\n",
        "        clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "        # Predict labels on the validation set\n",
        "        y_pred = clf.predict(X_test_tfidf)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        # Check if this combination of hyperparameters is the best\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_hyperparameters = {'depth': depth, 'iterations': iterations}\n",
        "\n",
        "# Print the best hyperparameters and accuracy\n",
        "print(\"Best Hyperparameters:\", best_hyperparameters)\n",
        "print(\"Best Accuracy:\", best_accuracy)\n",
        "\n",
        "# Create and train the final model with the best hyperparameters\n",
        "final_clf = CatBoostClassifier(**best_hyperparameters, verbose=0, random_state=12)\n",
        "final_clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Save the final model to a file using pickle\n",
        "with open('final_model.pkl', 'wb') as file:\n",
        "    pickle.dump(final_clf, file)\n"
      ],
      "metadata": {
        "id": "29y1_l22ccIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_test, random_forest_predictions)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', cbar=False)\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix - Random Forest Classifier')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "68xZt9b_quaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/code/qunminhle/hate-speech-detection-bert-for-classification/notebook"
      ],
      "metadata": {
        "id": "vH6CwNmwseJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Set the multiprocessing start method\n",
        "multiprocessing.set_start_method('fork')\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'max_features': ['auto', 'sqrt']\n",
        "}\n",
        "\n",
        "# Initialize RandomForestClassifier\n",
        "random_forest_clf = RandomForestClassifier()\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=random_forest_clf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(X_train_vec, y_train)\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Get the best model\n",
        "best_random_forest = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on test set\n",
        "best_random_forest_predictions = best_random_forest.predict(X_test_vec)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Best Random Forest Classifier Report:\")\n",
        "print(classification_report(y_test, best_random_forest_predictions))\n"
      ],
      "metadata": {
        "id": "fdG6shYaVtl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Extract the 'tweet' column from the dataframe\n",
        "tweets = df['tweet']\n",
        "\n",
        "# Define the categories for clustering\n",
        "categories = {\n",
        "    'Violence': ['violence', 'violent', 'assault', 'hate attack','rape','punch'],\n",
        "    'Sexism': ['sexism', 'gender discrimination', 'misogyny', 'objectification','slut','sex','sexy','hoe','pussy'],\n",
        "    'Racism': ['racism', 'racial discrimination', 'hate crime','nigga','black','niggers','ghetto'],\n",
        "    'Offensive' : ['bitch']\n",
        "\n",
        "}\n",
        "\n",
        "# Function to assign a category label to each tweet based on its content\n",
        "def assign_category(tweet):\n",
        "    for category, keywords in categories.items():\n",
        "        if any(keyword in tweet.lower() for keyword in keywords):\n",
        "            return category\n",
        "    return 'Non-Hate'\n",
        "\n",
        "# Apply the function to assign category labels to each tweet\n",
        "df['cluster'] = df['tweet'].apply(assign_category)\n",
        "\n",
        "# Print the number of tweets in each cluster\n",
        "print(\"Number of tweets in each cluster:\")\n",
        "print(df['cluster'].value_counts())\n"
      ],
      "metadata": {
        "id": "N5kga0sbsfIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the percentage of tweets in each cluster\n",
        "cluster_counts = df['cluster'].value_counts()\n",
        "total_tweets = len(df)\n",
        "\n",
        "cluster_percentages = (cluster_counts / total_tweets) * 100\n",
        "\n",
        "# Print the percentage of tweets in each cluster\n",
        "print(\"Percentage of tweets in each cluster:\")\n",
        "print(cluster_percentages)\n"
      ],
      "metadata": {
        "id": "kDcZQ6IjZRUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d-mcB_OxZRdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rB9HQtHwZRhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming you have a DataFrame df with 'tweet' column\n",
        "\n",
        "# Define the categories for clustering\n",
        "categories = {\n",
        "    'Violence': ['violence', 'violent', 'assault', 'hate attack', 'rape', 'punch'],\n",
        "    'Sexism': ['sexism', 'gender discrimination', 'misogyny', 'objectification', 'slut', 'sex', 'sexy', 'hoe', 'pussy'],\n",
        "    'Racism': ['racism', 'racial discrimination', 'hate crime', 'nigga', 'black', 'niggers', 'ghetto'],\n",
        "    'Offensive': ['bitch']\n",
        "}\n",
        "\n",
        "# Function to assign a category label to each tweet based on its content\n",
        "def assign_category(tweet):\n",
        "    for category, keywords in categories.items():\n",
        "        if any(keyword in tweet.lower() for keyword in keywords):\n",
        "            return category\n",
        "    return 'Other'\n",
        "\n",
        "# Apply the function to assign category labels to each tweet\n",
        "df['cluster'] = df['tweet'].apply(assign_category)\n",
        "\n",
        "# Initialize the TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Transform the tweets into TF-IDF features\n",
        "tfidf_matrix = vectorizer.fit_transform(df['tweet'])\n",
        "\n",
        "# Reduce the dimensionality of the TF-IDF features to 2D using PCA\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_pca = pca.fit_transform(tfidf_matrix.toarray())\n",
        "\n",
        "# Initialize KMeans with different values of n_clusters\n",
        "k_values = [2, 3, 4, 5, 6]\n",
        "best_score = -1\n",
        "best_k = None\n",
        "best_labels = None\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    labels = kmeans.fit_predict(X_pca)\n",
        "    score = silhouette_score(X_pca, labels)\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_k = k\n",
        "        best_labels = labels\n",
        "\n",
        "# Plot the clusters\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i in range(best_k):\n",
        "    plt.scatter(X_pca[best_labels == i, 0], X_pca[best_labels == i, 1], label=f'Cluster {i+1}')\n",
        "plt.title('2D Visualization of Tweet Clusters Using DBSCAN')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JrQ7P4zeeMxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count the number of tweets in each cluster\n",
        "cluster_counts = df['cluster'].value_counts()\n",
        "\n",
        "# Plot the bar graph\n",
        "plt.figure(figsize=(8, 6))\n",
        "cluster_counts.plot(kind='bar', color=['blue', 'green', 'red','black','yellow'])\n",
        "plt.title('Distribution of Tweets in Clusters')\n",
        "plt.xlabel('Cluster')\n",
        "plt.ylabel('Number of Tweets')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VJTLt2IY4wRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "\n",
        "# Initialize the TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Transform the tweets into TF-IDF features\n",
        "tfidf_matrix = vectorizer.fit_transform(tweets)\n",
        "\n",
        "# Reduce the dimensionality of the TF-IDF features to 2D using PCA\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_pca = pca.fit_transform(tfidf_matrix.toarray())\n",
        "\n",
        "# Plot the 2D representation of the clusters\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=df['cluster'], palette='Set1', s=100)\n",
        "plt.title('2D Visualization of Tweet Clusters')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5JCFs_ZF5Drn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}